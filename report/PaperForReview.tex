% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{array}  % Needed for p{} column types


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempLate.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{VORTEX: Vision Transformers for Interpretable Temporal Dating of Historical Paintings Through Ordinal Classification}

\author{David Song\\
University of Washington\\
1410 NE Campus Parkway Seattle, WA\\
{\tt\small davsong@uw.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Vibhav Peri\\
{\tt\small vperi@uw.edu}
}
\maketitle

\begin{abstract}

This research presents VORTEX (Vision Ordinal Regression Temporal EXtraction), combining Meta's DINOv2 Vision Transformer with Low-Rank Adaptation for predicting exact creation years of Western European paintings from 1600-1899. Our approach formulates year prediction as an ordinal classification problem using the CORAL framework, achieving a mean absolute error of 27.80 years on 50,781 paintings from major cultural institutions. The model's attention mechanism reveals period-specific visual cues that align with established art historical knowledge, providing transparent evidence for temporal attribution decisions while demonstrating that specialized architectures can achieve comparable performance to large-scale commercial models.

\end{abstract}

\section{Introduction}

The precise temporal placement of artworks represents a fundamental challenge in art history scholarship \cite{Mishory00}. Accurate dating influences virtually every aspect of research, from understanding individual artistic development to tracing the evolution of broader cultural movements \cite{Elgammal18}. 
% When researchers can confidently assign specific creation years to paintings, they unlock insights into how artistic innovations emerged, spread across geographic boundaries, and influenced subsequent generations of artists.

Despite centuries of scholarly effort and recent museum digitization initiatives, a substantial portion of the world's artistic heritage remains imprecisely dated. Major cultural institutions continue to struggle with temporal attribution, often cataloging significant works with vague temporal ranges such as ``first half of the 19th century'' rather than specific years.\footnote{See, for example, \url{https://collections.louvre.fr/en/ark:/53355/cl010064491}.} This imprecision creates challenges for digital humanities research, limiting our ability to understand chronological relationships between artworks.

\subsection{Current Challenges in Art Historical Dating}

Traditional dating methods each have inherent limitations. Connoisseurship remains subjective and often contentious among experts. Scientific analysis techniques require significant resources and often yield broad date ranges. Moreover, curators must invest substantial manual effort to narrow broad time ranges into single years—a process that is labor-intensive and difficult to scale.

While computational methods offer potential to augment traditional approaches, previous efforts have primarily focused on related problems such as style classification and artist attribution, not directly addressing precise temporal dating.
%with the interpretability necessary for scholarly acceptance.

\subsection{Objectives and Technical Approach}

We introduce \textbf{VORTEX} (\textbf{V}ision \textbf{O}rdinal \textbf{R}egression \textbf{T}emporal \textbf{EX}traction) to address the unique challenges of exact year prediction for historical paintings. Our approach targets Western European paintings from 1600 to 1899, encompassing major artistic movements from Baroque through early Impressionism. We formulate two core objectives:

\begin{enumerate}[leftmargin=1.2em,label=(\arabic*)]
    \item \textbf{Precise temporal prediction}: Develop a model capable of predicting exact creation years with mean absolute errors suitable for practical curatorial applications.

    \item \textbf{Interpretable decision-making}: Implement attention-based visualization techniques that reveal which visual elements inform temporal predictions, providing art historians with understandable evidence.
\end{enumerate}

To achieve these objectives, we leverage DINOv2 as our backbone, implement LoRA for parameter-efficient fine-tuning, and formulate year prediction as an ordinal classification problem using CORAL.

% \subsection{Expected Impact}

% From a technical perspective, we demonstrate that fine-tuning smaller open-weight pre-trained models can rival the performance of large SOTA multimodal language models for domain-specific computer vision tasks. This suggests that specialized architectures with efficient adaptation methods can achieve competitive results without requiring massive computational resources.

% From an art historical perspective, our emphasis on interpretability through attention analysis provides a crucial bridge between computational predictions and scholarly practices. By visualizing which visual elements inform temporal decisions, we enable art historians to evaluate model predictions against their domain expertise, building trust in computational methods within the humanities.

\section{Related Work}

The application of computer vision techniques to art historical problems has evolved significantly over the past two decades, with most research focusing on painting attribute prediction tasks rather than precise temporal dating.

\subsection{Evolution of Computer Vision in Art Analysis}

Early approaches utilized hand-crafted features like SIFT and HOG for style classification and artist identification \cite{Karayev14}. The introduction of CNNs marked a significant paradigm shift, with Karayev et al. demonstrating that deep features from AlexNet could effectively recognize artistic styles. This led to sophisticated frameworks like OmniArt \cite{Strezoski17OmniArt, Strezoski18}, which advanced multi-task learning for analyzing multiple artistic attributes including period classification.

The recent emergence of Vision Transformers offer advantages through global self-attention mechanisms that capture long-range dependencies in visual data. DINOv2 models \cite{Oquab23}, trained through self-supervised learning on massive visual datasets, have demonstrated exceptional transfer capabilities suitable for specialized domains with scarce annotated examples.

\subsection{Temporal Analysis in Art History Datasets}

No prior research has focused specifically on predicting exact creation years for historical paintings at scholarly precision levels. However, several initiatives included temporal prediction as a subtask within broader art analysis challenges.

The Rijksmuseum Challenge \cite{Mensink14} provided 112,000 images spanning diverse media including sculptures, decorative arts, and ceramics with creation years, achieving MAEs of approximately 72 years using traditional computer vision methods. The OmniArt Challenge \cite{Strezoski18} expanded this with 432,000 artworks across multiple artistic media in a multi-task framework, with best-performing ResNet-50 models achieving MAEs of 70.1 years.

The lack of existing benchmarks specifically designed for precise year-level dating of paintings represents a significant gap, as current datasets either mix multiple media types or treat temporal prediction as a secondary objective.

\section{Dataset}

Unlike broader art analysis tasks that have established benchmarks, temporal dating at scholarly precision levels represents an underexplored research direction with no dedicated evaluation frameworks. To address this gap, we systematically aggregated and standardized paintings from multiple authoritative sources to construct a novel dataset suitable for machine learning applications.



\subsection{Source Aggregation}

To construct our dataset, we carefully evaluated prior work to identify sources that meet our research requirements. We aggregated paintings from four major collections, as shown in Table \ref{tab:dataset_comparison}. Appendix \ref{fig:subfig-a} contains a further granular breakdown of the dataset.

% \begin{enumerate}[leftmargin=1.2em,label=(\arabic*)]
%     \item The Joconde database provides metadata for French museum artworks \cite{JocondeTerms}.
%     \item WikiArt offers a user-curated collection across multiple movements \cite{WikiArtTerms}. 
%     \item The Web Gallery of Art focuses on European paintings from Renaissance through 19th century \cite{WGATerms}.
%     \item The Rijksmuseum collection contains Dutch and Flemish paintings \cite{Mensink14}.
% \end{enumerate}

\begin{table*}[ht]
\centering
\caption{Dataset comparison across relevant features for year prediction of artworks.}
\label{tab:dataset_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Feature \ Dataset} & \textbf{Wiki Art}\cite{WikiArtTerms} & \textbf{WGA}\cite{WGATerms} & \textbf{Joconde}\cite{JocondeTerms} & \textbf{Rijks}\cite{Mensink14} & \textbf{\underline{All Datasets}}\\
\midrule
Number of Artworks           &       28997        &         10000      &       11954        &    1506     & 50781      \\
Publicly Available           &     \checkmark          &          \checkmark     &        \checkmark       &      \checkmark  & \checkmark       \\
Year Label Granularity $^{\dagger}$ &       0.2861        &       0.0096        &     0.0          &       0.0    & 0.1600    \\
Earliest Year                &        1600       &      1600         &      1600         &      1600      &  1600  \\
Latest Year                  &     1899          &        1899       &      1899         &       1899  & 1899      \\
Geographic Scope             &      \makecell{Western \\ Europe/America}         &       \makecell{Western \\ Europe/America}        &     \makecell{Predominantly \\ France}          &        Western Europe    & \makecell{Western \\ Europe/America} \\
\bottomrule
\end{tabular}
\noindent $^{\dagger}$ Average year 'window' range
\end{table*}

This multi-source approach mitigates collection biases inherent in any single institution while maximizing coverage across our 300-year temporal range. It enables the model to learn more robust temporal features from diverse examples of how different regions and schools evolved during the same time periods.

\subsection{Data Processing}

\paragraph{Filtering} Our dataset artwork inclusion criteria impose the following constraints:
\begin{itemize}
    \item Works must be explicitly identified as paintings (encompassing oil, tempera, watercolor, and mixed media techniques)
    \item Works must originate from Western European contexts
    \item Each work must include a digitized image
    \item Each work must possess either an exact year annotation or a date range with uncertainty less than 10 years
\end{itemize}
We exclude drawings, prints, sculptures, and decorative arts to maintain focus on painted works where temporal stylistic evolution follows consistent patterns. See Appendix \ref{fig:subfig-e} for example images from the dataset.

\paragraph{Label Standardization} Our standardization process addresses the variety of dating conventions found in art historical documentation. We accept paintings with exact year labels directly. For paintings with date ranges under 10 years (e.g., "1650-1655"), we use the midpoint as the exact year. Works with dating uncertainty exceeding 10 years are excluded to maintain precision in our ground truth labels.

\paragraph{Deduplication} Following the methodology established by Mao et al. \cite{Mao17}, we encode all images using SHA1 hashing to identify and remove duplicate artworks that may appear across multiple source collections. This process reduced our dataset from 52,457 to 50,781 unique paintings.

\subsection{Sampling Strategy}

The temporal distribution reflects both historical factors and contemporary digitization priorities. As shown in Figure \ref{fig:subfig-a}, earlier centuries show lower representation due to fewer surviving works and selective digitization focusing on major artists. The 19th century demonstrates significantly higher representation, particularly for Impressionist and Post-Impressionist works.

We deliberately avoid artificially balancing the temporal distribution through downsampling. Real-world applications encounter similar biases, with institutions holding proportionally more recent works.
% Training on this natural distribution ensures performance metrics accurately reflect expected deployment scenarios.
We partition the dataset using an 80/10/10 split for training, validation, and testing respectively.

All dataset construction adheres to source licensing requirements, with metadata preserved to ensure reproducibility while respecting copyright constraints.

\section{Methodology}

Our technical approach combines vision transformer architectures with parameter-efficient training strategies and ordinal classification for precise year prediction.

\subsection{Vision Transformer Foundation}

We employ Meta's DINOv2-Base model \cite{Oquab23} as our foundational feature extractor. DINOv2 is built on the Vision Transformer (ViT) architecture, which divides input images into fixed-size patches that are linearly embedded and processed as sequences. The DINOv2-Base variant (ViT-B/14 distilled) processes images as $14\times14$ pixel patches through 12 transformer blocks with an embedding dimension of 768 and 12 attention heads.

Vision Transformers often outperform CNNs on artwork analysis because their self-attention mechanisms can capture global relationships across the entire canvas, identifying long-range stylistic relationships that local convolutional filters might miss \cite{Dosovitskiy20ViT, Oquab23}.

\subsection{Parameter-Efficient Fine-Tuning with LoRA}

We implement LoRA \cite{Hu21LoRA} to address the computational challenges of fine-tuning on specialized art historical datasets. For each weight matrix $W_0 \in \mathbb{R}^{d \times k}$, we introduce low-rank adaptations:
$$W = W_0 + \Delta W = W_0 + BA$$
where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d, k)$.

We incorporate dropout in the LoRA layers to prevent overfitting and treat both LoRA rank $r$ and scaling factor $\alpha$ as hyperparameters to be optimized during training.

\subsection{Ordinal Classification Framework}

We recognize year prediction as fundamentally an ordinal problem and adopt the CORAL framework \cite{Cao20Ordinal}. For years $y_1 < y_2 < ... < y_{300}$ spanning 1600 to 1899, we predict:
$$P(Y > y_k | X) \text{ for } k = 1, ..., 299$$

CORAL ensures rank consistency through weight sharing across binary classifiers while maintaining individual bias terms, guaranteeing $P(Y > y_k) \geq P(Y > y_{k+1})$ for all $k$. During inference, we pass the probabilities through a sigmoid and determine the predicted year by identifying the transition point.

\subsection{Training Configuration}

We employed Optuna for hyperparameter optimization across 10 trials, treating LoRA rank, LoRA alpha multiplier, LoRA dropout, learning rate, and weight decay as tunable hyperparameters. We utilized a Tree-structured Parzen Estimator (TPE) sampler and a Successive Halving Pruner to enable efficient exploration of the hyperparameter space while pruning unpromising trials early.

The optimal parameters identified were: learning rate of 8.223285036834301e-05, weight decay of 1.6891276106598307e-05, LoRA rank of 8, alpha multiplier of 4, batch size of 4, and LoRA dropout of 0.11614437832577207. However, empirical testing revealed that a larger learning rate of approximately 8.22e-4 yielded superior performance. Final optimization employed Hugging Face's AdamW optimizer with default configurations.

For training, we utilize CORAL loss to enforce ordinal relationships between years, while Mean Absolute Error (MAE) serves as our primary metric for model selection and evaluation, as it provides an intuitive measure of average dating precision in years. We compute both metrics at each 1000 steps to monitor convergence.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/train_loss.png}
    \caption{Training progress over epochs. Left axis shows CORAL loss for both training (blue) and evaluation (orange). Right axis shows evaluation MAE in years (green).}
    \label{fig:training_curves}
\end{figure}

As shown in Figure \ref{fig:training_curves}, both training and evaluation losses decrease steadily through training. The evaluation MAE follows a similar pattern but with more variance, settling around 28 years by the last epoch. This convergence behavior suggests the model successfully learns temporal features without overfitting.

\subsection{Interpretability Through Attention Analysis}

We employ Attention Rollout \cite{Abnar20AttentionRollout} to aggregate attention weights across transformer layers, producing interpretable attention maps. This addresses key questions about model behavior: Do attention patterns align with established art historical knowledge? Does the model discover novel visual cues for temporal classification? How do attention patterns differ between successful and failed predictions? By examining these patterns, we build understanding of model capabilities and limitations, providing the transparency essential for scholarly acceptance of computational methods.

\section{Experimental Evaluation}

Our experimental evaluation assesses VORTEX's performance through quantitative metrics and qualitative analysis. We evaluated our approach on 5,079 test samples to determine whether VORTEX can match or exceed the performance of existing SOTA tools while providing superior interpretability.

\subsection{Evaluation Metrics}

As described in the training configuration, we employ CORAL loss during training and MAE for evaluation, with the latter providing an intuitive measure of temporal accuracy that art historians can readily interpret.

\subsection{Baseline Comparisons}

We evaluate VORTEX against commercial SOTA systems to establish performance benchmarks. Notably, we do not compare with the OmniArt Challenge as it treated temporal prediction as a secondary subtask and included non-painting artworks in its evaluation, making direct comparison inappropriate for our painting-specific approach.

\paragraph{Gemini 2.0 Flash} We benchmark against Google's SOTA multimodal large language model \cite{GeminiTeam23}. To ensure fair comparison, we prompt the model to focus solely on visual analysis without metadata:
\begin{quote}
    \textit{User: Closely examine this Western European painting. Only consider the painting itself. DO NOT USE ANY METADATA. Think carefully about what artistic movement it could be a part of and who the painter could be. Using these two attributes and any additional details about the painting, predict the exact year it was painted.}
\end{quote}

This comparison establishes whether specialized architectures with efficient fine-tuning can compete with general-purpose AI systems that leverage billions of parameters.

\subsection{Quantitative Results}

Our quantitative analysis confirms that VORTEX achieves performance comparable with large-scale commercial models. On the 5,078 test samples, VORTEX achieved a Mean Absolute Error (MAE) of \textbf{27.80 years}, with \textbf{2.2\%} (110 paintings) of its predictions being the exact year. For comparison, Google's Gemini 2.0 Flash model achieved an MAE of \textbf{21.39 years}, with an exact prediction rate of \textbf{15.2\%} (772 paintings).

As shown in the violin plot (Figure~\ref{fig:violin_plot}), while Gemini 2.0 Flash achieves a lower MAE with errors concentrated near zero, it exhibits a longer tail of exceptionally large errors, whereas VORTEX shows a more constrained error distribution despite a slightly higher median error.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/violin_plot.png}
        \caption{Violin plot showing the distribution of absolute prediction errors for VORTEX and Gemini 2.0 Flash. Gemini's errors are more concentrated near zero but have a longer tail, while VORTEX's errors are more tightly bounded.}
        \label{fig:violin_plot}
    \end{subfigure}
    \vfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/scatter_plot.jpg}
        \caption{Scatter plot of absolute prediction error against the true year. VORTEX's errors (red) show distinct linear artifacts corresponding to boundary predictions, while Gemini's large errors (blue) are more stochastically distributed.}
        \label{fig:scatter_plot}
    \end{subfigure}
    \caption{Comparative error analysis of VORTEX and Gemini 2.0 Flash.}
    \label{fig:error_plots}
\end{figure}

The scatter plot of errors over time (Figure~\ref{fig:scatter_plot}) further demonstrates these differences. Gemini's high-magnitude errors (e.g., $>200$ years) appear scattered across the timeline. In contrast, VORTEX's errors reveal a systematic pattern. The most prominent errors form two distinct linear artifacts, a direct result of the CORAL ordinal classification framework. When the model fails catastrophically, its prediction collapses to the boundaries of the dataset's date range (e.g. 1600 or 1899). This creates a primary failure line defined by $\text{Absolute Error} = \text{True Year} - 1600$ and a secondary, fainter line at $\text{Absolute Error} = 1899 - \text{True Year}$.
% This architectural feature means VORTEX's maximum error is bounded by the 300-year scope of the dataset, whereas Gemini, lacking such a constraint, produced errors exceeding 400 years.

\subsection{Interpretability Analysis}

To understand the models' underlying reasoning, we analyzed their best and worst-performing predictions. This qualitative assessment reveals that while both models achieve comparable MAEs, their decision-making processes and failure modes are fundamentally different, underscoring the complexity of the temporal dating task.

\paragraph{Model Success Analysis}
In cases of perfect prediction, both models demonstrated sophisticated analytical capabilities. Gemini achieved a 0-year error on a work from 1892 (Figure~\ref{fig:predictions}), providing nuanced reasoning that identified not only the Post-Impressionist movement but also the specific artist and medium:
\begin{quote}
    \textit{"The painting features a bold, graphic style... indicating the painting might be by Toulouse-Lautrec and is an example of post-impressionism and the distinctive characteristics of his poster work... The image and artistic style strongly resemble those of works created around the peak of that movement."}
\end{quote}
This shows an ability to synthesize style, subject, and technical details to pinpoint a work in time.

VORTEX also achieved a perfect prediction on an 1889 portrait (Figure~\ref{fig:predictions}). The attention map reveals its strategy: it focused intensely on high-information regions, particularly the subject's stiff white cuffs and head. The model learned to correlate the painting style and fashion of the era with the late 19th century. This painting's visual cues were all stylistically consistent with its creation year, allowing the model to make a precise attribution.

This analysis demonstrates that although both models achieve similar MAEs, their underlying reasoning differs. Gemini leverages a broad, context-aware understanding, while VORTEX excels at correlating specific, learned visual features with time.
% Both approaches achieve comparable accuracy, yet both are vulnerable to stylistically complex artworks, highlighting the profound difficulty of this task.

\paragraph{Model Failure Analysis}
For its worst prediction, Gemini 2.0 Flash erred by \textbf{441 years}, dating a work from 1861 to 1420 (Figure~\ref{fig:predictions}). Its chain-of-thought reasoning reveals a logically sound process based on flawed historical premises:
\begin{quote}
    \textit{"The painting appears to be a study or preparatory sketch... The style suggests a medieval or early Renaissance influence, characterized by the figure with a halo and the somber expression. Based on these elements, I would place it within the early Renaissance period."}
\end{quote}
The model correctly identified the Early Renaissance \textit{style} but failed to recognize the artwork as a 19th-century creation by Edward Burne-Jones, a leader of the Pre-Raphaelite Brotherhood, which intentionally emulated this earlier style. The error stems from a lack of art-historical context about revivalist movements, not a failure of visual analysis.

VORTEX's worst prediction, an error of \textbf{278 years} (dating an 1898 painting to 1620), resulted from a similar deception by stylistic anachronism. The painting is an example of late 19th-century Historicism, emulating the grand manner of the Baroque period. The attention rollout (Figure~\ref{fig:predictions}) shows the model correctly focused on period-specific features like the cherubic putto and dramatic drapery—common in 17th-century art. This overwhelming evidence for an earlier style caused the CORAL classifier to output 1620, a predictable failure mode for our architecture when faced with intentionally misleading stylistic cues.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering

% --- Best Predictions Row ---
(a) \textbf{Best Predictions} \\[0.5em]
\begin{tabular}{cc}
    \includegraphics[width=0.45\linewidth]{images/best_vortex.jpg} &
    \includegraphics[width=0.4\linewidth]{images/best_gemini.jpg} \\
    \makecell{\textbf{VORTEX}\\ (0 year error) \\ Ground Truth: 1889} & \makecell{\textbf{Gemini Flash 2.0}\\ (0 year error) \\ Ground Truth: 1892} \\
\end{tabular}

\vspace{0.5em}

% \noindent\begin{minipage}{0.5\textwidth}
% \small
% \textbf{Chain-of-Thought Reasoning:} \\ 
% Gemini: \textit{"The painting appears to be a study or preparatory sketch due to the visible underdrawing and unfinished quality. The style suggests a medieval or early Renaissance influence, characterized by the figure with a halo and the somber expression. Based on these elements, I would place it within the early Renaissance period."}\\

% VORTEX: See attention map.

% \end{minipage}

\vspace{2em}

% --- Worst Predictions Row ---
(b) \textbf{Worst Predictions} \\[0.5em]
\begin{tabular}{cc}
    \includegraphics[width=0.5\linewidth]{images/worst_vortex.jpg} &
    \includegraphics[width=0.35\linewidth]{images/worst_gemini.jpg} \\
    \makecell{\textbf{VORTEX}\\ (290 yrs error)\\ Ground Truth: 1898} & \makecell{\textbf{Gemini Flash 2.0} \\ (441 yrs error)\\ Ground Truth: 1861} \\
\end{tabular}

\vspace{0.5em}

% \noindent\begin{minipage}{0.5\textwidth}
% \small
% \textbf{Chain-of-Thought Reasoning:} \\ 
% Gemini: \textit{"The painting features a bold, graphic style with a focus on capturing the essence of the subject rather than precise realism. The subject matter seems theatrical, indicating the painting might be by Toulouse-Lautrec and is an example of post-impressionism and the distinctive characteristics of his poster work for the Moulin Rouge. The image and artistic style strongly resemble those of works created around the peak of that movement. I can also see the use of broad brushstrokes, limited color palette, and simplified forms contribute to an expressive aesthetic."}\\

% VORTEX: See attention map.

% \end{minipage}

\caption{Best and worst predictions by VORTEX and Gemini Flash 2.0. (a) Top row shows correctly predicted years. (b) Bottom row shows worst errors, along with the model's reasoning.}
\label{fig:predictions}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

Our experimental evaluation reveals both the promise and limitations of computational approaches to art historical dating, providing insights that inform both technical development and scholarly practice.

\subsection{Implications for Art History Scholarship}

The performance and failure modes of both VORTEX and Gemini 2.0 Flash demonstrate that these AI tools cannot replace expert scholarship but serve as powerful supplementary instruments. 
% For curatorial applications, VORTEX provides immediate practical value by rapidly suggesting temporal attributions for new acquisitions and offering independent evidence for disputed dates.
The attention visualizations provide interpretable evidence that curators can evaluate against their expertise, building trust through transparency.

The ability to process entire collections enables new forms of temporal analysis at computational scale, allowing researchers to analyze trends across thousands of paintings, identify periods of rapid stylistic innovation, and discover unrecognized temporal patterns. This opens new methodological possibilities for quantitative art history.

\subsection{Limitations}

Several limitations constrain the current approach. The focus on Western European paintings from 1600-1899 represents a specific cultural and temporal scope, and learned features may not transfer effectively to other artistic traditions. Artistic movements often blend together and manifest differently across regions, while reproductions of earlier works can exhibit visual characteristics of their original period despite being created centuries later. Our model relies solely on painting images, ignoring potentially valuable contextual metadata that human experts routinely consider.

\subsection{Future Work}

Several promising directions could extend our approach. Benchmarking against human experts would provide the definitive performance standard, while comparison with state-of-the-art CNNs would clarify Vision Transformer advantages. Systematic evaluation of commercial multimodal systems would establish comprehensive baselines.

Our findings suggest art historical analysis constitutes a high-rank task, with consistent improvements observed at larger LoRA ranks. Scaling to larger DINOv2 variants could yield substantial accuracy gains, as capturing subtle stylistic variations requires rich representational capacity.

Multimodal integration represents the natural evolution, incorporating textual metadata, technical analysis results, and iconographic descriptions alongside visual features. This mirrors how human experts synthesize diverse evidence when dating artworks, suggesting a path toward more robust automated systems.

\section{Conclusion}

This research establishes that precise computational dating of historical paintings can be achieved through architectures balancing technical performance with scholarly requirements. VORTEX demonstrates that Vision Transformers with parameter-efficient fine-tuning and ordinal classification provide year-level temporal predictions with 27.80-year MAE suitable for curatorial applications.

The attention-based interpretability features enable art historians to examine visual evidence underlying predictions, creating collaborative frameworks rather than replacing human expertise. This capability enables quantitative analysis across collections, tracking stylistic evolution and understanding cultural exchange patterns.

Future development should address expanding temporal and geographic scope, benchmarking against human experts and commercial models, and incorporating multimodal information. VORTEX's success demonstrates that specialized approaches can rival general-purpose systems for domain-specific tasks while maintaining the balance between technical innovation and humanistic scholarly values.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage


\begin{figure*}[t]
\section*{Appendix}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/Distribution of Western Paintings in all Datasets by Year (1600-1899) all decades big.png}
        \caption{Distribution of Western Paintings Collected Across all Datasets by Decade (1600-1900)}
        \label{fig:subfig-a}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/WikiArt_s Style pie chart big.png}
        \caption{WikiArt 'Style' Pie Chart}
        \label{fig:subfig-b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/WGA_s SCHOOL pie chart big.png}
        \caption{WGA 'School' Pie Chart}
        \label{fig:subfig-c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/Joconde's Ecole_pays pie chart big.png}
        \caption{Joconde 'Ecode/Pays' Pie Chart}
        \label{fig:subfig-d}
    \end{subfigure}

    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/all datasets example imgs better.png}
        \caption{Example Images Collected Over Multiple Years}
        \label{fig:subfig-e}
    \end{subfigure}

    \caption{(a) This shows the amount of images we have collected over all datasets per decade between 1600 and 1900. Also displays the mean year (~1801) and the median year (~1837) (b) Shows the 'style' classification of the images collected from WikiArt. The pie chart was created only considering images from WikiArt that fit our temporal and geographical filters. (c) Shows the 'school' classification of all images collected from WGA (d) Shows the school/country classification of all images collected from Joconde. and (e) An example of a few images collected in our dataset across the years. The year displayed is the year the painting was created or the median year if there was a range of $<10$. You can clearly see a shift in style and subjects in these images as centuries pass.}
    \label{fig:combined-figure}
\end{figure*}

\end{document}